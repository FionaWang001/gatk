\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page since and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{bm}
\usepackage{amsfonts}

% Note the substitute command needs to be enclosed in double brackets
% otherwise the bold font spills out of the command
\newcommand{\vzero}{{\bf 0}}
\newcommand{\va}{{\bf a}}
\newcommand{\vI}{{\bf I}}
\newcommand{\vb}{{\bf b}}
\newcommand{\vd}{{\bf d}}
\newcommand{\vf}{{\bf f}}
\newcommand{\vc}{{\bf c}}
\newcommand{\vp}{{\bf p}}
\newcommand{\vv}{{\bf v}}
\newcommand{\vz}{{\bf z}}
\newcommand{\vn}{{\bf n}}
\newcommand{\vm}{{\bf m}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vG}{{\bf G}}
\newcommand{\vQ}{{\bf Q}}
\newcommand{\vM}{{\bf M}}
\newcommand{\vW}{{\bf W}}
\newcommand{\vX}{{\bf X}}
\newcommand{\vPsi}{{\bf \Psi}}
\newcommand{\vSigma}{{\bf \Sigma}}
\newcommand{\vlambda}{{\bf \lambda}}
\newcommand{\vpi}{{\bm \pi}}
\newcommand{\vtheta}{{\bm \theta}}
\newcommand{\valpha}{{\bm{\alpha}}}
\newcommand{\vLambda}{{\bf \Lambda}}
\newcommand{\vA}{{\bf A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\LL}{\mathbb{L}}



\title{Mutect2 Orientation Artifact Filter}
\author{Takuto Sato}


\begin{document}
\maketitle

\section{The probabilistic model}

Our probabilistic graphical model is in Figure (\ref{fig:pgm}). For each reference context $c$ - that is, for every $k$-mer in the reference - the discrete latent variable $z$ represents the state of the site with respect to the degree of orientation-bias 

\begin{equation*}
z \in \{ \text{F1R2}_a, \text{F2R1}_a, \text{Hom Ref}, \text{Germline Het}, \text{Somatic Het}, \text{Hom Var} \}
\end{equation*}

where $a$ is the set of possible SNP alt alleles (e.g. if reference context is AGT, then $a \in \{ A, C, T \}$). Thus we have $|\vz| = 10$. We use the lowercase $k$ as index into $\vz$.

$z \sim \mathrm{Categorical}(\vpi)$, where $\pi_{k}$ is the prior probability of the kth component of $z$ given allele $a$ and satisfies $\sum_k \pi^a_{ck} = 1$, where $k \in [1, |Z|]$. The number of alt alleles at the site, $m$, is a binomial random variable whose probability of success $f$ (i.e. allele fraction) depends on the state $z$. We explicitly model the number of alt alleles because its dependence on $z$ encapsulates our knowledge that, when orientation bias is present, we \textit{always} have (artifactual) alt reads at the site. In other words, if we do not observe alt reads at all (i.e. $z = \mathrm{Hom ref}$), we cannot have orientation bias at the site. \todo[inline, color = green]{This is not a valid asusmption i.e. We cannot assume that $m = 0$ and $z = \text{hom ref}$ go together. Rather, we must design the model such that this is the case. That is, we give it a very low likelihood when $m = 0$ and $z$ is anything other than hom ref.}

We then model the number of $F1R2$ reads among $m$ alt reads as $x|m,z \sim \mathrm{Binom(\vtheta_z)}$, parameterized by $\theta_z$ that depends on whether the site suffers from orientation bias. In other words, $\theta$ depends on $z$. 

\begin{itemize}
\item $z \sim \mathrm{Categorical}(\vpi^a_c)$
\item $m | z \sim \mathrm{Binom(f_z)}$
\item $x | m,z \sim \mathrm{Binom(\vtheta_z)}$
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pgm.png}
\caption{\label{fig:pgm} The probabilistic graphical model for the read orientation filter}
\end{figure}

% TODO: What does the model know? What doesn't it know? The model does not know about the normal sample. If there's a low allele fraction variant in 
% Work on my strength, not weaknesses. 

%%%
\subsection{EM algorithm}
We estimate the hyperparameters $\Theta = \{ \vpi, \vf, \vtheta \}$ that maximize likelihood using the EM algorithm. We can then compute the posterior probability of the latent variable $z$ to determine whether a variant is an orientation artifact and thus should be filtered. We do not learn the hyperparameters to the beta distribution $\alpha$ and $\beta$.

\subsubsection{E step}
In the E step, we compute the responsibilities $\gamma_{nk} = p(z_{nk} | x_n, m_n, a)$. $\gamma_{nk}$ is the posterior probability of the $k$th component of $z$. Using Bayes rule we may compute the unnormalized responsibilities $\gamma^*_{nk}$ as follows

\begin{align}
\gamma^*_{nk} &\propto p(z_{nk} | x_n, m_n) \nonumber \\
		        &\propto p(z_{nk}) p(m_n | z_{nk}, f_{k} ) p(x_n | m_n ) \nonumber \\
		        &=  \Big \{ \begin{array}{ll}
        				\pi_{k} \cdot \mathrm{BetaBinom}(m_n | \alpha, \beta, R) \cdot \mathrm{Binom}(x_n | \theta_k, m_n) 	&\text{if } z_{nk} = \text{Somatic Het} \\
			        \pi_{k} \cdot \mathrm{Binom}(m_n | f_k, r_n) \cdot \mathrm{Binom}(x_n | \theta_k, m_n) 	&\text{otherwise} \label{eq:posterior}
			     \end{array}
\end{align}

where $r$ is the number of reads at site $n$, and the asterisk in $\gamma^*_{nk}$ denotes that the responsibility is not normalized and thus is not a probability. Normalizing the responsibilities is straightforward:

\begin{align}
\gamma_{nk} = \frac{\gamma^*_{nk}}{\sum_{k'} \gamma^*_{nk'}} \label{eq:normalize}
\end{align}

\subsubsection{M step}
In the M step, we maximize the expectation of the complete-data log likelihood $\LL (\Theta) = \ln p(\vx, \vm, \vz | \vpi, \vp, \vtheta)$ with respect to the posterior distribution over $z$ (so $\E$ is a shorthand for $\E_{\vz \sim p(\vz|\vx,\vm)}$). 

\begin{align}
\E [ \LL &(\Theta) ] = \E [ \ln p(\vx, \vm , \vz | \vpi, \vf, \vtheta) ] \nonumber \\
           &= \E \Big [ \ln \prod_{n = 1}^{N}  p(z_n) p(m_n | z_n, f_{k} ) p(x_n | m_n, z_n, \theta_k ) \Big ] \nonumber \\
           &= \E \Big [ \ln \prod_{n = 1}^{N} \Big \{ \prod_{k = 1}^{K} \pi_{k}^{z_{nk}} \Big \} \Big \{ \prod_{k \neq k^*} \mathrm{Binom}(m_n | f_k, r_n)^{z_{nk}} \mathrm{BetaBinom}(m_n | \alpha, \beta, r_n)^{z_{nk^*}} \Big \} \Big \{ \prod_{k = 1}^{K} \mathrm{Binom}(x_n | \theta_k, m_n) ^{z_{nk}} \Big \} \Big ] \notag \\
            &= \E \Big [ \sum_{n = 1}^{N} \sum_{k \neq k^*} z_{nk} \Big ( \ln \pi_{k}  + m_n \ln f_k + (r_n - m_n) \ln (1-f_k) + x_n \ln \theta_k + (m_n - x_n) \ln (1 - \theta_k) \Big )  + \notag \\
            &\phantom{--} z_{nk^*} \Big ( \ln \pi_{k^*} + \ln B(m_n + \alpha, r_n - m_n + \beta) - \ln B(\alpha, \beta) + x_n \ln \theta_{k^*} + (m_n - x_n) \ln (1 - \theta_{k^*}) \Big ) \Big ] + const \nonumber \\
            &= \sum_{n = 1}^{N} \sum_{k \neq k^*} \gamma_{nk} \Big ( \ln \pi_{k}  + m_n \ln f_k + (r_n - m_n) \ln (1-f_k) + x_n \ln \theta_k + (m_n - x_n) \ln (1 - \theta_k) \Big )  + \notag \\
            &\phantom{--} \gamma_{nk^*} \Big ( \ln \pi_{k^*} + \ln B(m_n + \alpha, r_n - m_n + \beta) - \ln B(\alpha, \beta) + x_n \ln \theta_{k^*} + (m_n - x_n) \ln (1 - \theta_{k^*}) \Big ) + const  \label{eq:expjointLL}
\end{align} 

where $k^*$ is the index into $z$ such that $z_{nk^*} = \text{Somatic Het}$, $K = |Z|$, $\E [ z_{nk} ] = \gamma_{nk}$, and $B(\cdot, \cdot)$ is the beta function. $const$ denotes all terms that do not depend on the hyperparameters $\vpi, \vf, \vtheta$. 

We also define the following to simplify the math that follows. $N_k$ may be interpreted as the effective number of samples in state $z_k$. Interpretations for $\bar{x}_k$, $\bar{m}_k$, and $\bar{r}_k$ are analogous.

\begin{align}
N_k      &= \sum_n \gamma_{nk} \label{eq:nk} \\
\bar{x}_k  &= \frac{1}{N_k} \sum_n \gamma_{nk} x_n \label{eq:xbar} \\
\bar{m}_k &= \frac{1}{N_k} \sum_n \gamma_{nk} m_n \label{eq:mbar} \\
\bar{r}_k  &= \frac{1}{N_k} \sum_n \gamma_{nk} r_n \label{eq:rbar}
\end{align}



We now take the derivative of (\ref{eq:expjointLL}) with respect to $\theta_k, p_k, \pi_{k}$, set them equal to 0, and solve for those parameters to get the update equations. First we take the derivative with respect to $\vtheta$;

\begin{align}
\frac{\partial}{\partial \theta_k} \E [ \LL (\Theta)] &= \sum_n \gamma_{nk} \Big( \frac{ x_n}{\theta_k} - \frac{m_n - x_n}{1- \theta_k} \Big)\\
					         							     &= \frac{N_k \bar{x}_k}{\theta_k} - \frac{N_k \bar{m}_k - N_k \bar{x}_k}{1- \theta_k} = 0
\end{align}

where we used (\ref{eq:xbar}) and (\ref{eq:mbar}). Solving for $\theta_k$ gives us

\begin{align}
\hat{\theta}_k = \frac{\bar{x}_k}{\bar{m}_k} \label{eq:theta}
\end{align}

Thus we update $\theta_k$ to the mean F1R2 alt reads divided by the mean alt depth, weighting contribution from each example by the current estimates of responsibilities $\gamma_{nk}$. Taking the derivative with respect to $f_k$ is analogous to $\theta_k$:

\begin{align}
\hat{f}_k = \frac{\bar{m}_k}{\bar{r}_k} \label{eq:f}
\end{align}

Next we take the derivative with respect to $\pi_{k}$ under the constraint $\sum_k \pi_{ak} = 1$ using a Lagrange multiplier:

\begin{align}
\frac{\partial}{\partial \pi_{k}} \Big( \E [ \LL (\Theta) ] + \lambda ( \sum_k \pi_{k} - 1) \Big) &= \sum_{n} \frac{\gamma_{nk}}{\pi_{k}} + \lambda \nonumber \\
																   &= \frac{N_{ak}}{\pi_{ak}} + \lambda = 0 \label{eq:dpi}
\end{align}

where $\lambda$ is the Lagrange multiplier. Rearranging (\ref{eq:dpi}) and summing over $k$, we get

\begin{align*}
\sum_k N_{k} &= \sum_k - \lambda \pi_{k} \\
      N &= - \lambda
\end{align*}

Plugging in for $\lambda$ in (\ref{eq:dpi}) gives us

\begin{equation}
\hat{\pi}_{k} = \frac{N_{k}}{N}
\end{equation}

\subsection{Inference}
Once we learn the hyperparameters that maximize model evidence $\hat{\Theta} = \{ \hat{\vpi}, \hat{\vf}, \hat{\vtheta} \}$, for each candidate variant we may compute the posterior probability of $z_k$ by evaluating (\ref{eq:posterior}) using $\hat{\Theta}$ and normalizing as shown in (\ref{eq:normalize}). 


%%%%%%%%%%
\section{July 14th meeting with David}

When building a model, we'd like to know what information the model knows about. The previous model knew that 

\begin{itemize}
\item the probability of orientation bias depends on context
\item if orientation bias, most \underline{alt} reads have one or the other orientation
\end{itemize}

But the model did not know, crucially, that if there's orientation bias, there must be some alt reads. Since we were looking at sites that have alt reads, the model didn't know 

We then decided to add the variable $m$ to the model.  Once the model was updated, we went over different scenarios: what would happen if 
\begin{itemize}
\item $n = 100, m = 10, x = 5$ 
\item $n = 100, m = 1, x = 1$
\item $n = 100, m = 0, x = 0$
\end{itemize}

How does each part of the model vote, so to speak, for or against such data? The different components are $p(m|z), p(x|m), p(z | \pi)$. The first case rules 
When the later likelihoods don't favor one of the other, the final vote falls on $p(z | \pi)$ which, assuming we already have the hyperparameters learned over all sites, uses the prior across all sites to vote one way or another. That is, when data is not very informative, when it doesn't tell us one way or another, we use the prior knowledge to make the decision, which makes sense.

When mean field theory is used, the conjugacy across nodes is not crucial. Conjugacy between immediate neighbors makes life easier.

$m$ could be binomial. Or it could be beta-binomial. Beta-binomial has larger spread when $n$ is large or something. 

At one point I suggested that $m|z = \mathrm{no bias}$ be a mixture, between somatic variant (where $m$ can be large) or hom ref (where $m$). Instead I could just add more values to the $z$ variable such that $z \in \{ \text{F1R2}, \text{F2R1}, \text{Balanced Hom Ref}, \text{Balanced Somatic}, \text{Germline} \}$.

%%%%%%%%%%
\section{Previous approach, and why my filter is better}
\begin{itemize}
\item The old tool adds up the count of pro, con, ref, alt for each transition over different contexts, so if there's any context-specific information we lose it.
\item 
\end{itemize}




\end{document}